require 'strscan'

class Sharpnesh::Parser
  # Tokenize input with state
  class Lexer
    include TokenType

    def initialize(io, name)
      @scanner = StringScanner.new(io.read)
      @name = name
      @tokens = []
      @next = 0
      @line = 1
      @col = 1
      @rules_stack = []
    end

    # return next token
    #
    # @return [Token]
    def next(*expected, allow_blank: true)
      return if !(token = peek(*expected, allow_blank: allow_blank))
      @next += 1
      token
    end

    # return next token (not steped)
    #
    # @return [Token]
    def peek(*expected, allow_blank: true)
      token = if @next < @tokens.size
        @tokens[@next]
      else
        tokenize
      end
      (expected.empty? || expected.include?(token.type)) && (allow_blank || token.blank.empty?) ? token : nil
    end

    def accept(pattern, type, allow_blank: true)
      if @next + 1 < @tokens.size
        raise "cannot call `accept` when 2 or more tokens are buffered (head of buffer is `#{@tokens[@next].body}`)"
      end

      if @next + 1 == @tokens.size
        last_body = @tokens.pop.body
        @col -= last_body.length
        @scanner.pos -= last_body.bytesize
      end

      rollback_pos = @scanner.pos
      blank = allow_blank ? @scanner.scan(/[ \t]*/) : ''
      if !(body = @scanner.scan(pattern))
        @scanner.pos = rollback_pos
        return
      end
      @col += body.length + blank.length
      token = Token.new(type, body, blank, @line, @col)
      @tokens << token
      @next += 1
      token
    end

    # back to previous token
    #
    def back
      raise 'cannot back empty lexer' if @next.zero?
      @next -= 1
    end

    # return whether all tokens are consumed
    #
    # @return [Boolean]
    #
    def eos?
      @next > 0 && @tokens[@next - 1].type == TK_EOS
    end

    def use_rules(rules)
      @rules_stack.push(rules)
      begin
        yield
      ensure
        @rules_stack.pop
      end
    end

    private

    DEFAULT_RULES = [
      { pattern: /([^$|&;()<> \t\n]|\\[$|&;()<> \t"'])+/, method: :on_token, opt: TK_STR },
      { pattern: /;/, method: :on_token, opt: TK_SEMICOLON }
    ].freeze

    def tokenize
      blank = @scanner.scan(/[ \t]*/)
      @col += blank.length
      return Token.new(TK_EOS, blank, nil, @line, @col) if @scanner.eos?

      @rules_stack.last.each do |pattern:, method:, opt:|
        matched = @scanner.scan(pattern)
        return send(method, matched, blank, opt) if matched
      end

      raise ParseError, "cannot recognize charactor `#{@scanner.rest[0]}`"
    end

    def on_token(body, blank, type)
      token = Token.new(type, body, blank, @line, @col)
      @col += body.length
      @tokens << token
      token
    end

    # Token infomation generated by Lexer
    #
    class Token
      attr_reader :type, :body, :blank, :start_line, :start_col

      def initialize(type, body, blank, start_line, start_col)
        @type = type
        @body = body
        @blank = blank
        @start_line = start_line
        @start_col = start_col
      end
    end
  end
end
